}
# Initialise a vector of length n and calculate the mean of each observation across the
# nsamp samples.
studentisedredm=numeric(n)
for(i in 1:n){
studentisedredm[i]=mean(studentisedred[i,])
}
# Plot a Q-Q plot, paying attention to the normality of the data.
qqnorm(studentisedredm,xlim=c(-3,3),ylim=c(-3,3),lwd=2)
qqline(studentisedredm,col=2,lwd=2)
# Plot studentised residuals against index, looking for any patterns.
par(mfrow=c(1,1))
plot(seq_along(studentisedredm),studentisedredm,xlab="Index",
ylab="Bayesian studentised residual",ylim=c(-3,3))
abline(h=0)
# Calculate the posterior mean fitted values and plot against residuals
# with a horizontal line at y=0.
fittedvaluesm=numeric(n)
for(i in 1:n){
fittedvaluesm[i]=mean(fittedvalues[i,])
}
plot(fittedvaluesm,studentisedredm,xlab="Fitted value",ylab="Bayesian Studentised residual")
abline(h=0)
# Allocate priors for the 3 new models.
prec.prior.weibull <- list(alpha = list(prior = "pc.alphaw", param = 5))
prec.prior.gamma <- list(prec = list(prior = "loggamma", param = c(1,0.01)))
prec.prior.lognormal <- list(prec = list(prior = "loggamma", param = c(1,0.01)))
# Gamma
m1_gamma <- inla(rainfall ~ windspeed + temperature + year, data=data,
family = "gamma",
control.family=list(hyper=prec.prior.gamma),
control.fixed=prior.beta,
control.compute = list(config = TRUE, waic = TRUE, cpo = TRUE))
# Weibull
m1_weibull <- inla(rainfall ~ windspeed + temperature + year, data=data,
family = "weibull",
control.family=list(hyper=prec.prior.weibull, variant = 1),
control.fixed=prior.beta,
control.compute = list(config = TRUE, waic = TRUE, cpo = TRUE))
# Lognormal
m1_lognormal <- inla(rainfall ~ windspeed + temperature + year, data=data,
family = "lognormal",
control.family=list(hyper=prec.prior.lognormal),
control.fixed=prior.beta,
control.compute = list(config = TRUE,waic = TRUE, cpo = TRUE))
# Calculate the goodness-of-fits (NSLCPO) with the Gaussian model as a baseline.
m1_gaussian_NSLCPO = -sum(log(m1_gaussian$cpo$cpo))
m1_gamma_NSLCPO = -sum(log(m1_gamma$cpo$cpo))
m1_weibull_NSLCPO = -sum(log(m1_weibull$cpo$cpo))
m1_lognormal_NSLCPO = -sum(log(m1_lognormal$cpo$cpo))
# Statistics of NSLCPO and WAIC as a dataframe.
model_statistics = data.frame(model = c("Gaussian", "Gamma", "Weibull", "LogNormal"),
WAIC = c(m1_gaussian$waic$waic, m1_gamma$waic$waic, m1_weibull$waic$waic,m1_lognormal$waic$waic),
NSLCPO = c(m1_gaussian_NSLCPO, m1_gamma_NSLCPO, m1_weibull_NSLCPO, m1_lognormal_NSLCPO))
model_statistics
# Select 10,000 examples from the posterior of the Weibull model
nsamp <- 10000
n <- nrow(data)
y<- data$rainfall
m1.weibull.samples=inla.posterior.sample(n=nsamp, result=m1_weibull)
# Get the linear predictor, eta, before converting to sigma for rweibull. Also,
# get the alpha values.
eta = ((inla.posterior.sample.eval(function(...) {(Predictor)},
m1.weibull.samples)))
sigma = 1/exp(eta)
alpha = inla.posterior.sample.eval(function(...) {theta}, m1.weibull.samples)
# Create nsamp samples for each observation in yrep.
yrep=matrix(0,nrow=n,ncol=nsamp)
for(row.num in 1:n){
yrep[row.num, ]<- rweibull(n = nsamp, shape = alpha[,], scale = sigma[row.num,])
}
# Statistics of interest in this case. N.B. fBasics required for skewness
yrepmax=apply(yrep,2,max)
yrepmin=apply(yrep,2,min)
yrepmedian=apply(yrep,2,median)
yrepmean=apply(yrep,2,mean)
yrepskewness=apply(yrep,2,skewness)
# Predictive checks using replicated data for the above statistics.
hist(yrepmin,col="gray40")
abline(v=min(y),col="red",lwd=2)
hist(yrepmax,col="gray40")
abline(v=max(y),col="red",lwd=2)
hist(yrepmedian,col="gray40")
abline(v=median(y),col="red",lwd=2)
hist(yrepmean,col="gray40")
abline(v=mean(y),col="red",lwd=2)
hist(yrepskewness,col="gray40")
abline(v=skewness(y),col="red",lwd=2)
# Each year between 1940 and 2022 is represented in all 12 months so
# let's add a column to represent this as an int.
data$year_int <- rep(1940:2021, each = 12)
# Not including leap years in calculations
days_in_month <- c(31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31)
# The monthly rainfull is therefore these multiplied
data$monthly.rainfall <- data$rainfall * rep(days_in_month,82)
# Aggregate the sums of the rainfall by the year column we made
aggregated_rain <- aggregate(data$monthly.rainfall, list(data$year_int), sum)
# Create a new model but including the year 2022 and predict the rainfall
# with the control.predictor argument.
m1_weibull_NA <- inla(rainfall ~ windspeed + temperature + year, data=NA_data,                           family = "weibull",
control.family=list(hyper=prec.prior.weibull, variant = 1),
control.fixed=prior.beta,
control.compute = list(config = TRUE),
control.predictor = list(compute = TRUE))
# Create nsamp samples from the posterior
nsamp = 20000
m1_weibull_NA_samples=inla.posterior.sample(n=nsamp, result=m1_weibull_NA)
# Repeat the same calculations as above to get replicates but only for
# 2022 this time, iterating over the 12 rows of 2022 only.
eta  <- inla.posterior.sample.eval(function(...) {Predictor[985:996]}, m1_weibull_NA_samples)
sigma = 1/exp(eta)
alpha = inla.posterior.sample.eval(function(...) {theta}, m1_weibull_NA_samples)
yrep=matrix(0,nrow=12,ncol=nsamp)
for(row.num in 1:12){
yrep[row.num, ]<- rweibull(n = nsamp, shape = alpha[,], scale = sigma[row.num,])
}
# Get the total rainfall for each month and take the sum across samples
# to get yearly totals. This can be done through recycling days_in_month.
total_rain_yrep <- yrep * days_in_month
total_rain_2022 <- colSums(total_rain_yrep)
# The proportion of these posterior samples above the previous maximum
# will be considered the probability.
print(sum(total_rain_2022 > max(aggregated_rain$x))/nsamp)
#Load in the data
frogs<-read.csv("Frogs.csv")
# Load all required libraries for Q2.
require(rjags)
require(coda)
require(MASS)
# Select a subset of the data including the frogs with IDs 1 and 46 alongside the
# MFCCs_.2 and MFCCs_11 columns represented by column indices 1 and 10.
frogs.subset <- frogs[frogs$RecordID==1 | frogs$RecordID==46, c(1,10)]
# We want to calculate observations as column vectors so take the transpose.
y <- t(frogs.subset)
# Define the JAGS model string.
model_string <- "
model {
# Priors
mu ~ dmnorm(c(0, 0), mu_precision[1:2,1:2])
rho ~ dunif(-1, 1)
tau[1] ~ dgamma(1, 0.1)
tau[2] ~ dgamma(1, 0.1)
# Likelihood
for (i in 1:n) {
y[1:2,i] ~ dmnorm(mu[1:2], T[1:2,1:2])
}
# Variance and SD Calculations from Tau.
sigma_sq[1] <- 1 / tau[1]
sigma_sq[2] <- 1 / tau[2]
sigma[1] <- sqrt(sigma_sq[1])
sigma[2] <- sqrt(sigma_sq[2])
# Precision Matrix
common_factor <- 1/(1-rho^2)
T[1,1] <- common_factor / sigma_sq[2]
T[2,2] <- common_factor / sigma_sq[1]
T[1,2] <- - (rho * common_factor) / (sigma[1] * sigma[2])
T[2,1] <- T[1,2]
}
"
# Initialise the JAGS model, supplying the precision for mu as data to avoid
# reassigning in every iteration.
model <- jags.model(textConnection(model_string), data=list(y=y, n = ncol(y),
mu_precision = diag(2) * 0.0001), n.chains=1)
# Burn-in phase
update(model, 4000)
# Sampling
samples <- coda.samples(model, variable.names=c("mu", "rho", "sigma_sq"),
n.iter=10000,thin=5)
# Print model summary alongside the posterior density plots and trace plots
summary(samples)
plot(samples)
# Initialize the JAGS model, making sure to specify 3 chains.
model <- jags.model(textConnection(model_string),
data=list(y=y, n = ncol(y), mu_precision = diag(2) * 0.0001),
n.chains=3)
# Burn-in phase of 4000.
update(model, 4000)
# Take 7500 samples of mu and rho and keep only every 5.
samples <- coda.samples(model, variable.names=c("mu", "rho"), n.iter=7500, thin=5)
effectiveSize(samples)
# The acf estimates for each of our parameters can be found after combining
# chains (as.matrix).
acf(as.matrix(samples)[,"rho"])
acf(as.matrix(samples)[,"mu[1]"])
acf(as.matrix(samples)[,"mu[2]"])
# Get the Gelman-Rubin statistics and plots.
gelman.diag(samples)
gelman.plot(samples)
model_string <- "
model {
# Priors
for (k in 1:2) {
mu_temp[1, k] ~ dnorm(0, 0.0001)
mu_temp[2, k] ~ dnorm(0, 0.0001)
}
rho ~ dunif(-1, 1)
tau[1] ~ dgamma(1, 0.1)
tau[2] ~ dgamma(1, 0.1)
# Probabilities
for (i in 1:n) {
p[i,1] ~ dbeta(0.5,0.5)
p[i,2] <- 1 - p[i,1]
}
# Likelihood
for (i in 1:n) {
Z[i] ~ dcat(p[i,1:2])
y[1:2,i] ~ dmnorm(mu[1:2,Z[i]], T[1:2,1:2])
}
# Sort mu[1,1:2] as an identifiability constraint.
mu[1,1:2] <- sort(mu_temp[1, 1:2])
mu[2, 1:2] <- mu_temp[2,1:2]
# Variance and SD calculations from Tau.
sigma_sq[1] <- 1 / tau[1]
sigma_sq[2] <- 1 / tau[2]
sigma[1] <- sqrt(sigma_sq[1])
sigma[2] <- sqrt(sigma_sq[2])
# Precision Matrix
common_factor <- 1/(1-rho^2)
T[1,1] <- common_factor / sigma_sq[2]
T[2,2] <- common_factor / sigma_sq[1]
T[1,2] <- - (rho * common_factor) / (sigma[1] * sigma[2])
T[2,1] <- T[1,2]
}
"
# Make sure we scale the data so each column has mean 0 and sd 1.
frogs.subset.scaled <- scale(frogs.subset)
# Initialise the model, using just 1 chain this time.
model <- jags.model(textConnection(model_string), data=list(y=t(frogs.subset.scaled),
n = ncol(t(frogs.subset.scaled))), n.chains=1)
# Use 5000 burn in samples to update the model.
update(model, 5000)
# Generate 2500 samples of sigma/rho, making sure to select every fifth before getting
# the summary and plotting.
samples <- coda.samples(model, variable.names=c("sigma", "rho"), n.iter=2500, thin=5)
summary(samples)
plot(samples)
# Generate 2500 samples of the latent labels and probabilities seperately,
# using thin = 5 again.
samples2 <- coda.samples(model, variable.names=c("Z[1]","Z[48]"), n.iter=2500, thin=5)
samples3 <- coda.samples(model, variable.names=c("p[1,1]","p[48,2]"), n.iter=2500,thin=5)
# Get the summary and plot the samples seperately.
summary(samples2)
plot(samples2)
summary(samples3)
plot(samples3)
model_string <- "
model {
# Priors
for (k in 1:K) {
mu_temp[1, k] ~ dnorm(0,0.0001)
mu_temp[2, k] ~ dnorm(0,0.0001)
}
rho ~ dunif(-1, 1)
tau[1] ~ dgamma(1, 0.1)
tau[2] ~ dgamma(1, 0.1)
# Probabilities and alpha prior
for (k in 1:K){
alpha[k] ~ dunif(0.99,1.01)
}
for (i in 1:n) {
p[i, 1:K] ~ ddirch(alpha[])  # Dirichlet prior for p_i
}
# Likelihood
for (i in 1:n) {
Z[i] ~ dcat(p[i,1:K])
y[1:2,i] ~ dmnorm(mu[,Z[i]], T[1:2,1:2])
}
# Mu identifiability constraint
mu[1,1:K] <- sort(mu_temp[1, 1:K])
mu[2,1:K] <- sort(mu_temp[2, 1:K])
# Variances and SDs from tau
sigma_sq[1] <- 1 / tau[1]
sigma_sq[2] <- 1 / tau[2]
sigma[1] <- sqrt(sigma_sq[1])
sigma[2] <- sqrt(sigma_sq[2])
# Precision Matrix
common_factor <- 1/(1-rho^2)
T[1,1] <- common_factor / sigma_sq[2]
T[2,2] <- common_factor / sigma_sq[1]
T[1,2] <- - (rho * common_factor) / (sigma[1] * sigma[2])
T[2,1] <- T[1,2]
}
"
# Add the extra frog to the subset data and scale before use in models.
frogs.subset2 <- frogs[frogs$RecordID==1 | frogs$RecordID==46| frogs$RecordID==56, c(1,10)]
frogs.subset2.scaled <- scale(frogs.subset2)
# Initialise models of 2 chains with K=2, 3, and 4.
model_2 <- jags.model(textConnection(model_string),
data=list(y=t(frogs.subset2.scaled),
n = ncol(t(frogs.subset2.scaled)),
K = 2),
n.chains=2)
model_3 <- jags.model(textConnection(model_string),
data=list(y=t(frogs.subset2.scaled),
n = ncol(t(frogs.subset2.scaled)),
K = 3),
n.chains=2)
model_4 <- jags.model(textConnection(model_string),
data=list(y=t(frogs.subset2.scaled),
n = ncol(t(frogs.subset2.scaled)),
K = 4),
n.chains=2)
# Use a burn in of 10,000.
update(model_2, 10000)
update(model_3, 10000)
update(model_4, 10000)
# Use dic.samples with n.iter = 5000 and output the values.
dic_values_2 <- dic.samples(model_2, n.iter = 5000)
dic_values_3 <- dic.samples(model_3, n.iter = 5000)
dic_values_4 <- dic.samples(model_4, n.iter = 5000)
dic_values_2
dic_values_3
dic_values_4
N=nrow(frogs)
subset.data=frogs[frogs$RecordID==1 | frogs$RecordID==46 |frogs$RecordID==56,c(1,10)]
frog1.ids=1:sum(frogs$RecordID==1)
frog46.ids=(sum(frogs$RecordID==1)+1):(sum(frogs$RecordID==1)+sum(frogs$RecordID==46))
frog56.ids=(1:N)[-c(frog1.ids,frog46.ids)]
plot(subset.data)
points(subset.data, col=c(rep(1,length(frog1.ids)),
rep(2,length(frog46.ids)),
rep(3,length(frog56.ids))), main="True frog type",asp=1)
# Create a function for the mode as this isn't built in.
mode <- function(x) {
# Get the unique values in a vector x
unique_x <- unique(x)
# Get a table containing the number of times each unique value shows up in x
tabulated_x <- tabulate(match(x, unique_x))
# Get the maximum frequency in this table
max_frequency <- max(tabulated_x)
# This will return the mode and use the first if more than one.
return(unique_x[tabulated_x == max_frequency][1])
}
# Get 2500 samples of Z
samples_3 <- coda.samples(model_3, variable.names=c("Z"), n.iter=2500)
# Combine the samples of the chains and get the modes of each observation for
# the two MFCCs.
clusters_combined <- apply(as.matrix(samples_3), 2, mode)
# Plot the data and colour them based on the cluster latent labels obtained.
plot(subset.data)
points(subset.data, col=clusters_combined, main="True frog type",asp=1)
#setwd("/Users/cyrusseyrafi/srs-assignment/assignment_2")
setwd("C:/Users/BCapo/Desktop/University of Edinburgh Masters/Sem 2/srs-assignment/assignment_2")
# NOTES #
# Q1 is the 20% of areas with lowest participation in higher education
# Need to make all columns but INSTITUTION_NAME numeric.
# Canterbury Christ Church Uni seems to have a too high total???
######                            IMPORTS                               ######
library(ggplot2)
library(glmnet)
######                       READ DATA + CLEAN                          ######
# Read the data and read the first few rows.
data = read.csv("data.csv")
head(data)
# Remove 'X.1' column and 'X' cols, R provides indices for us.
data = data[,-c(1,2,3)]
head(data)
str(data)
# FIX CONTINUATION BY LOOKING AT IT:
data$continuation
# APPEARS TO BE "n/a" converting it to string column
data$continuation <- gsub("n/a", NA, data$continuation)
data$continuation <- as.numeric(data$continuation)
# Get column indices and names by using a named list/vector as a 'dictionary'.
cols = c(1:ncol(data))
names(cols) = names(data)
# Remove SIMD data as useless.
SIMD.cols <- which(substr(names(cols),1,4) == "SIMD")
data = data[,-SIMD.cols]
str(data)
# Reset the cols data.
cols = c(1:ncol(data))
names(cols) = names(data)
# NEED TO GET RID OF NA VALUES FIRST.
data <- na.omit(data)
cor(data[,-1])
#plot(data)
# Get the index for the data through the INSTITUTION_NAME
INSTITUTION_NAME <- data$INSTITUTION_NAME
rownames(data) <- INSTITUTION_NAME
######                                EDA                               ######
#pairs(data[,-1])
for(col in names(cols)[-1]){
print(col)
plotted <- ggplot(data[,-1], aes(x = .data[[col]], y = satisfied_feedback)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
theme_minimal()
print(plotted)
}
# POLAR4.Q4 bad cor but the rest are good?? Added value and total: bad. Could do
# feature engineering/transformations on ethnic/gender columns to improve but
# overall not great.
######                DATA CLEANING/FEATURE ENGINEERING?                ######
#other_ethnic_outlier <- data[which(data$Other.ethnic.group==max(data$Other.ethnic.group)),]
#other_ethnic_outlier
#apply(data[,cols[c("Men", "Women")]], 1, sum)
#apply(data[,cols[c("White.ethnic.group", "Black.ethnic.group",
#                   "Asian.ethnic.group", "Other.ethnic.group", "Mixed.ethnic.group")]], 1, sum)
#apply(data[,cols[c("POLAR4.Q1", "POLAR4.Q2", "POLAR4.Q4", "POLAR4.Q5", "POLAR4.Q3")]], 1, sum)
model_data <- data[cols[c("satisfied_feedback", "satisfied_teaching",
"students_staff_ratio", "spent_per_student",
"avg_entry_tariff", "career_after_15_month",
"continuation", "Women")]]
model_data[,-1] <- scale(model_data[,-1])
continuation_sq <- (model_data$continuation)^2
satisfied_teaching_sq <- (model_data$satisfied_teaching)^2
model_data <- cbind(model_data, continuation_sq, satisfied_teaching_sq)
# Get column indices and names by using a named list/vector as a 'dictionary'.
model_cols = c(1:ncol(model_data))
names(model_cols) = names(model_data)
######                                MODELS                            ######
# RESIDUAL PLOTS
residual_plots <- function(model, data, response) {
# Extract predicted values.
predicted <- predict(model, data, type = "response")
# Compute residuals.
residuals <- data[[response]] - predicted
# Standardize residuals.
sigma_hat <- sd(residuals)
residuals <- residuals / sigma_hat
# Initialize plots.
par(mfrow = c(1, 3))
## Residuals vs. Fitted plot.
plot(predicted, residuals, main = "Residuals vs. Predicted",
xlab = "Predicted Values", ylab = paste("Residuals"), pch = 19)
abline(h = 0, col = "red", lty = 2)
## Histogram of Residuals.
hist(residuals, breaks = 30, probability = TRUE,
main = "Histogram of Residuals", xlab = paste("Residuals"))
curve(dnorm(x, mean = mean(residuals), sd = sd(residuals)), add = TRUE, col = "blue")
## Q-Q Plot.
qqnorm(residuals, main = "Q-Q Plot of Residuals")
qqline(residuals, col = "red", lwd = 2)
}
# BASLINE MODEL
baseline_model <- lm(satisfied_feedback ~ ., data = model_data)
summary(baseline_model)
par(mfrow = c(2,2))
plot(baseline_model)
i = 1
par(mfrow = c(2,2))
for(col in names(model_cols)[-1]){
if(i%%4 == 0){
par(mfrow = c(2,2))
}
plot(model_data[,col], residuals(baseline_model), xlab = col)
abline(h = 0)
}
# Drop Subject Specific Unis?
# model_data <- model_data[!(INSITUTION_NAME %in% c("SOAS ", "Goldsmiths ", "University of the Arts London ")),]
# INSTITUTION_NAME <- rownames(model_data)
# STEP MODEL
step_model <- step(baseline_model, direction = "both")
summary(step_model)
par(mfrow = c(2,2))
plot(step_model)
## STEP MODEL WITH 2ND ORDER INTERACTIONS
base_model_int <- lm(satisfied_feedback ~ (.)^2, data = model_data)
step_model_int <- step(base_model_int, direction = 'backward', trace = 0)
summary(step_model_int) ## higher R^2 (but probably too many coefficients) -> could use the terms in lasso to get the "most meaningful" ones
plot(step_model_int)
## function to create interactions manually for lasso
create.interactions <- function(interaction.names, data){
interactions <- interaction.names[grep(':', interaction.names)]
for (name in interactions){
name.split <- strsplit(name, ':')[[1]]
## get individual names
name1 <- name.split[1]
name2 <- name.split[2]
## add interaction (= product)
data[[name]] <- data[[name1]] * data[[name2]]
}
return(data)
}
model_data_glm <- create.interactions(names(coef(step_model_int)), model_data)
## step can lead to overfitting and unstable results
mod.glm.step <- cv.glmnet(x = as.matrix(model_data_glm[, -1]), y = model_data_glm$satisfied_feedback)
coef(mod.glm.step) ## get rid of less significant interactions/covariates
# BRMS (BAYESIAN REGRESSION MODELS USING STAN)
## can also use brms (bayesian regression using stan) and the skew normal distribution
## -> can capture the slightly skewed distribution
## use covariates from lasso (with interactions)
mod.brms <- brm(satisfied_feedback ~ satisfied_teaching + spent_per_student + avg_entry_tariff
+ career_after_15_month + continuation + continuation_sq + satisfied_teaching_sq
+ satisfied_teaching:continuation_sq + students_staff_ratio:Women
+ spent_per_student:career_after_15_month + avg_entry_tariff:continuation
+ continuation:Women + Women:satisfied_teaching_sq,
data = model_data_glm, family = skew_normal())
pairs(data[,-1])
i = 1
par(mfrow = c(2,2))
for(col in names(model_cols)[-1]){
if(i%%4 == 0){
par(mfrow = c(2,2))
}
plot(model_data[,col], residuals(baseline_model), xlab = col)
abline(h = 0)
}
model_data
data
data["University of Nottingham ",]
data["University of Nottingham",]
data["The University of Nottingham",]
data["The University of Nottingham",]
data["Nottingham ", ]
data["Edinburgh ", ]
#other_ethnic_outlier <- data[which(data$Other.ethnic.group==max(data$Other.ethnic.group)),]
#other_ethnic_outlier
#apply(data[,cols[c("Men", "Women")]], 1, sum)
apply(data[,cols[c("White.ethnic.group", "Black.ethnic.group",
"Asian.ethnic.group", "Other.ethnic.group", "Mixed.ethnic.group")]], 1, sum)
